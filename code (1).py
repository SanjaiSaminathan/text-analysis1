# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPomLZWNMt8oh51OPEPBPcCt5U13uu1p

# Student ID: XXX

**You student_id is your 7/8 digit faser number.**

This is a sample format for CE807-24-SP: Assignment . You must follow the format.
The code will have three broad sections, and additional section, if needed,


1.   Common Codes
2.   Method/model 1 Specific Codes
3.   Method/model 2 Specific Codes
4.   Other Method/model Codes, if any

**You must have `train_gen`, `test_gen` for Generative method  and `train_dis`, `test_dis` for Discriminatuve method to perform full training and testing. This will be evaluated automatically, without this your code will fail and no marked.**

You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices.

Before each `code block/function`, you must have a `text block` which explain what code block/function is going to do. For each function/class, you need to properly document what are it's input, functionality and output.

If you are using any non-standard library, you must have command to install that, for example `pip install datasets`.

You must print `train`, `validation` and `test` performance measures.

You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.

Your code must

*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**.
*   read dataset from './student_id/data/number/', where number is last digit of your student_id folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]
*   save model after finishing the training in './student_id/Model_Gen/' and './student_id/Model_Dis/' for Generative and Discriminative model respectively.
*   at testing time you will load models from './student_id/Model_Gen/' and './student_id/Model_Dis/'  for Generative and Discriminative model respectively. Your output file based on the test file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.
*  after testing, your output file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.




**Install and import all required libraries first before starting to code.**

Let's install all require libraries. For example, `transformers`
"""

!pip install transformers

"""Let's import all require libraries.
For example, `numpy`
"""

import numpy as np
import os
import pickle
import pandas as pd
import re


import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords

import nltk
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download('wordnet')
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

from nltk.corpus import stopwords as ind
nltk.download('punkt')

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.pipeline import Pipeline

"""**Let's put your student id as a variable, that you will use different places**"""

student_id = 2311895 # Note this is an interger and you need to input your id

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)

"""# Common Codes

In this section you will write all common codes, for examples


*   Data read
*   Command Line argument reading
*   Performance Matrics
*   Print Dataset Statistics
*   Saving model and output
*   Loading Model and output
*   etc

**Let's first allow the GDrive access and set data and model paths**

For examples,

student_id = 12345670

set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807-24-SP/Lab10/’ in your GDrive

now set all global variable,


Sample output directory and file structure: https://drive.google.com/drive/folders/1okgSzgGiwPYYFp7NScEt9MNVolOlld1d?usp=share_link
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Add your code to initialize GDrive and data and models paths

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '/content/gdrive/MyDrive/Text'
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '8') # Make sure to replace 0 with last digit of your student Regitration number
train_file = os.path.join(DATA_PATH, 'train.csv')
print('Train file: ', train_file)

val_file = os.path.join(DATA_PATH, 'valid.csv')
print('Validation file: ', val_file)

test_file = os.path.join(DATA_PATH, 'test.csv')
print('Test file: ', test_file)


MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number
MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory
print('Model Generative directory: ', MODEL_Gen_DIRECTORY)

MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'


MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory
print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)

MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'

"""Let's see train file"""

train_df = pd.read_csv(train_file)
train_df.head()

test_df = pd.read_csv(test_file)
test_df.head()

# @title comment_id

from matplotlib import pyplot as plt
train_df['comment_id'].plot(kind='line', figsize=(8, 4), title='comment_id')
plt.gca().spines[['top', 'right']].set_visible(False)

print(train_df)

"""Let's show you a sample output file. Notice all fields, `out_label` is your model's output for that `tweet` and `id`"""

# Remove the repated the Text.
def remove_unwanted(data):
  unwanted_ward = ['SDATA_8','NEWLINE_TOKENNEWLINE_TOKEN','NEVER ASSUME','EDATA_8']
  for word in unwanted_ward:
    data = data.replace(word, '')
  return data.strip()



# Do tokenization,Stemming,Lemmatization
def pre_processing(text):
    text_stemmer = PorterStemmer()
    text_lemmatizer = WordNetLemmatizer()
    # remove the tokenization
    pre_tokens = nltk.word_tokenize(text)
    # remove the Stemming
    #pre_tokens = [text_stemmer.stem(token) for token in pre_tokens]
    # remove the  Lemmatization
    pre_tokens = [text_lemmatizer.lemmatize(token) for token in pre_tokens]
    return ' '.join(pre_tokens)

# Remove the Stopwords

def remove_stopwords(text):
    stop_words = stopwords.words('english')
    text = " ".join([word for word in text.split() if word not in stop_words])
    return text


# Pre Processing
def pre_process_final(pre_data):

  #Remove label
  pre_data['comment'] = pre_data['comment'].apply(remove_unwanted)


  #Making column as string
  pre_data['comment'] = pre_data['comment'].astype(str)

  pre_data['comment'] = pre_data['comment'].str.lower()



  #To remove unicode
  pre_data['comment'] = pre_data['comment'].str.replace(r"(@\[[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", regex=True)

  # Remove the stopwords
  pre_data['comment'] = pre_data['comment'].apply(remove_stopwords)
  pre_data['comment'] = pre_data['comment'].apply(pre_processing)

  return pre_data

train_df = pre_process_final(train_df)
train_df.head()

# Test Data.
test_df = pre_process_final(test_df)
test_df.head()

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create a pipeline that includes vectorization and the SVM model
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', SVC(kernel='linear'))  # You can tune the kernel and other parameters
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')  # Can choose 'micro', 'macro', or 'weighted' based on what suits your needs

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Classification Report:\n", classification_report(y_test, y_pred))

pip install -U imbalanced-learn

!pip install --upgrade scikit-learn
!pip install --upgrade imbalanced-learn

from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline

#-----------------
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline  # To handle SMOTE with pipelines

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create a pipeline that includes SMOTE, vectorization, and the SVM model
pipeline = ImbPipeline([
    ('tfidf', TfidfVectorizer()),
    ('smote', SMOTE(random_state=42)),  # Apply SMOTE to the training data
    ('svm', SVC(kernel='linear'))  # You can tune the kernel and other parameters
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')  # Can choose 'micro', 'macro', or 'weighted' based on what suits your needs

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.pipeline import Pipeline

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create a pipeline that includes vectorization and the Linear Regression model
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('linear_regression', LinearRegression())  # Linear Regression model
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# As linear regression predicts continuous values, you might need to round the predictions to get binary classification
# For example, if the prediction is greater than 0.5, classify as toxic, else non-toxic
y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Classification Report:\n", classification_report(y_test, y_pred_binary))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create a pipeline that includes vectorization, SMOTE, and the Linear Regression model
pipeline = ImbPipeline([
    ('tfidf', TfidfVectorizer()),
    ('smote', SMOTE(random_state=42)),  # Adding SMOTE for oversampling
    ('linear_regression', LogisticRegression())  # Logistic Regression model
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create a pipeline that includes vectorization, SMOTE, and the Decision Tree model
decision_tree_pipeline = ImbPipeline([
    ('tfidf', TfidfVectorizer()),
    ('smote', SMOTE(random_state=42)),  # Adding SMOTE for oversampling
    ('decision_tree', DecisionTreeClassifier())  # Decision Tree model
])

# Train the decision tree model
decision_tree_pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred_dt = decision_tree_pipeline.predict(X_test)

# Evaluate the decision tree model
accuracy_dt = accuracy_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

print("Decision Tree Model:")
print("Accuracy:", accuracy_dt)
print("F1 Score:", f1_dt)
print("Classification Report:\n", classification_report(y_test, y_pred_dt))

# Create a pipeline for Random Forest
random_forest_pipeline = ImbPipeline([
    ('tfidf', TfidfVectorizer()),
    ('smote', SMOTE(random_state=42)),  # Adding SMOTE for oversampling
    ('random_forest', RandomForestClassifier())  # Random Forest model
])

# Train the Random Forest model
random_forest_pipeline.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = random_forest_pipeline.predict(X_test)

# Evaluate the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print("\nRandom Forest Model:")
print("Accuracy:", accuracy_rf)
print("F1 Score:", f1_rf)
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

pip install tensorflow

from keras.preprocessing.text import Tokenizer

import numpy as np
from sklearn.model_selection import train_test_split
#from sklearn.feature_extraction.text import Tokenizer
from sklearn.metrics import accuracy_score, f1_score, classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample data format: Assume 'comment' column for text and 'label' for classification
X = train_df['comment']
y = train_df['toxicity']

# Convert text data into sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)

# Get vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# Pad sequences to have consistent length
max_seq_length = 100  # Define the maximum sequence length
X_padded = pad_sequences(X_sequences, maxlen=max_seq_length)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=42)

# Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Define RNN model
rnn_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length),
    SpatialDropout1D(0.2),
    LSTM(128),
    Dense(1, activation='sigmoid')
])

# Compile the model
rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the RNN model
rnn_model.fit(X_train_resampled, y_train_resampled, epochs=5, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the RNN model
rnn_scores = rnn_model.evaluate(X_test, y_test, verbose=0)
print("RNN Model - Loss:", rnn_scores[0], "- Accuracy:", rnn_scores[1])

# Define LSTM model
lstm_model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length),
    SpatialDropout1D(0.2),
    LSTM(128),
    Dense(1, activation='sigmoid')
])

# Compile the model
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the LSTM model
lstm_model.fit(X_train_resampled, y_train_resampled, epochs=5, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the LSTM model
lstm_scores = lstm_model.evaluate(X_test, y_test, verbose=0)
print("LSTM Model - Loss:", lstm_scores[0], "- Accuracy:", lstm_scores[1])

"""We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"""

from sklearn.metrics import f1_score

def compute_performance(y_true, y_pred):
    """
    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also display Confusion Matrix with proper X & Y axis labels.
    Also, returns F1 score

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list


    Returns:
        float
    """

    accuracy = accuracy_score(y_true, y_pred)
    recall_macro = recall_score(y_true, y_pred, average='macro')
    precision_macro = precision_score(y_true, y_pred, average='macro')
    f1_macro = f1_score(y_true, y_pred, average='macro')

    # Print performance metrics
    print("Accuracy:", accuracy)
    print("Recall (macro):", recall_macro)
    print("Precision (macro):", precision_macro)
    print("F1 (macro):", f1_macro)

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.show()

    return float(f1_macro)

def save_model(model,model_dir):
  # save the model to disk
  # Check if the Model directory exists

  # Note you might have to modify this based on your requirement

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model_file = os.path.join(model_dir, 'model.sav')
  pickle.dump(model, open(model_file, 'wb'))

  print('Saved model to ', model_file)

  return model_file

def load_model(model_file):
    # load model from disk

    # Note you might have to modify this based on your requirement

    model = pickle.load(open(model_file, 'rb'))

    print('Loaded model from ', model_file)

    return model

"""# Let's download GDrive Link into a directory"""

import requests

def extract_file_id_from_url(url):
    # Extract the file ID from the URL
    file_id = None
    if 'drive.google.com' in url:
        file_id = url.split('/')[-2]
    elif 'https://docs.google.com' in url:
        file_id = url.split('/')[-1]

    return file_id

def download_file_from_drive(file_id, file_path):
    # Construct the download URL
    download_url = f"https://drive.google.com/uc?id={file_id}"

    # Download the file
    response = requests.get(download_url)
    if response.status_code == 200:
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print("File downloaded successfully!",file_path)
    else:
        print("Failed to download the file.")

def download_zip_file_from_link(file_url,file_path):

  file_id = extract_file_id_from_url(file_url)
  if file_id:
      download_file_from_drive(file_id, file_path)
  else:
      print("Invalid Google Drive URL.")

"""# Zip and Unzip a GDrive File"""

import zipfile
import shutil
import os

# Function to zip a directory
def zip_directory(directory, zip_filename):
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(directory):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))
        print('Created a zip file',zip_filename)

# Function to unzip a zip file
def unzip_file(zip_filename, extract_dir):
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print('Extracted a zip file to',extract_dir)

# Example usage:
# directory_to_zip = 'path/to/your/directory'
# zip_filename = 'output_zipfile.zip'

# # Zip the directory
# zip_directory(directory_to_zip, zip_filename)

# # Unzip the zip file
# extract_dir = 'path/to/extract'
# unzip_file(zip_filename, extract_dir)

"""# Get Sharable link of your Zip file in Gdrive"""

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


def get_gdrive_link(file_path):
    # Authenticate and create PyDrive client
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    # Find the file in Google Drive
    file_name = file_path.split('/')[-1]
    file_list = drive.ListFile({'q': f"title='{file_name}'"}).GetList()

    # Get the file ID and generate the shareable link
    if file_list:
        file_id = file_list[0]['id']
        gdrive_link = f"https://drive.google.com/file/d/{file_id}/view?usp=sharing"
        return gdrive_link
    else:
        return "File not found in Google Drive"

def get_shareable_link(url):

    file_id = extract_file_id_from_url(url)

    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    try:
        file_obj = drive.CreateFile({'id': file_id})
        file_obj.FetchMetadata()
        file_obj.InsertPermission({
            'type': 'anyone',
            'value': 'anyone',
            'role': 'reader'
        })

        # Get the shareable link
        return file_obj['alternateLink']
    except Exception as e:
        print("Error:", e)
        return None

# if __name__ == "__main__":
#     # Replace 'YOUR_FILE_ID' with the ID of the file you want to share
#     file_id = 'YOUR_FILE_ID'
#     shareable_link = get_shareable_link(file_id)
#     if shareable_link:
#         print("Shareable link:", shareable_link)
#     else:
#         print("Failed to generate shareable link.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score

pip install joblib

pip install --upgrade scikit-learn

"""# Method Generative Start

In this section you will write all details of your Method 1.

You will have to enter multiple `code` and `text` cell.

Your code should follow the standard ML pipeline


*   Data reading
*   Data clearning, if any
*   Convert data to vector/tokenization/vectorization
*   Model Declaration/Initialization/building
*   Training and validation of the model using training and validation dataset
*   Save the trained model
*   Load and Test the model on testing set
*   Save the output of the model


You could add any other step(s) based on your method's requirement.

After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.

## Training Generative Method Code
Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.

Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best.
"""

import numpy as np
from sklearn.model_selection import train_test_split
#from sklearn.feature_extraction.text import Tokenizer
from sklearn.metrics import accuracy_score, f1_score, classification_report
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences

def train_Gen(train_file, val_file, model_dir):


    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)



    # Sample data format: Assume 'comment' column for text and 'label' for classification
    X = train_df['comment']
    y = train_df['toxicity']

    # Convert text data into sequences
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(X)
    X_sequences = tokenizer.texts_to_sequences(X)



    # Get vocabulary size
    vocab_size = len(tokenizer.word_index) + 1

    # Pad sequences to have consistent length
    max_seq_length = 100  # Define the maximum sequence length
    X_padded = pad_sequences(X_sequences, maxlen=max_seq_length)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=42)

    # Apply SMOTE to balance the classes
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # Define RNN model
    rnn_model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length),
        SpatialDropout1D(0.2),
        LSTM(128),
        Dense(1, activation='sigmoid')
    ])

    # Compile the model
    rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Train the RNN model
    rnn_model.fit(X_train_resampled, y_train_resampled, epochs=5, batch_size=64, validation_data=(X_test, y_test))

    # Evaluate the RNN model
    rnn_scores = rnn_model.evaluate(X_test, y_test, verbose=0)
    print("RNN Model - Loss:", rnn_scores[0], "- Accuracy:", rnn_scores[1])

    save_model(pipeline,model_dir)
    # Now Zip Model to share it
    zip_directory(model_dir, MODEL_Dis_File)

    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

    print(model_gdrive_link)
    get_shareable_link(model_gdrive_link)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return model_gdrive_link

model_gdrive_link = train_Gen(train_file, val_file, MODEL_Gen_DIRECTORY)

"""## Testing Method 1 Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

def test_Gen(test_file, MODEL_PATH,model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model

    """

    print('\n Start by downloading model')


    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link,test_model_file)
    print('Model downloaded to', test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)
    # Load the saved model
    test_df = pd.read_csv(test_file)
    print('\n Data is loaded from ', test_file)

    # Let's get the model file name & load it
    # Note you have to use same name a you used in the save

    test_model_file = os.path.join(test_model_path, 'Model_Gen', 'model.sav')

    model = load_model(test_model_file)

    # Let's do the prediction using test data
    y_pred= model.predict(test_df['comment'])

    # Now save the model output in the same test file
    # Note the name of output column, this is for the discriminative model
    test_df['out_label_model_Gen'] = y_pred

    # Now save the model output in the same output file
    test_df.to_csv(test_file, index=False)
    print('\n Output is save in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return test_file

test_Gen(test_file, MODEL_PATH,model_gdrive_link)

"""## Method Generative End

# Method Discriminative Start

In this section you will write all details of your Method 2.

You will have to enter multiple `code` and `text` cell.

Your code should follow the standard ML pipeline


*   Data reading
*   Data clearning, if any
*   Convert data to vector/tokenization/vectorization
*   Model Declaration/Initialization/building
*   Training and validation of the model using training and validation dataset
*   Save the trained model
*   Load and Test the model on testing set
*   Save the output of the model

You could add any other step(s) based on your method's requirement.

After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.

## Training Method Discriminative Code
Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.

Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

def train_dis(train_file, val_file, model_dir):

  train_df = pd.read_csv(train_file)
  val_df = pd.read_csv(val_file)

  # Sample data format: Assume 'comment' column for text and 'label' for classification
  X = train_df['comment']
  y = train_df['toxicity']
  X_val = val_df['comment']
  y_val = val_df['toxicity']

  # Split the data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

  # Create a pipeline that includes SMOTE, vectorization, and the SVM model
  pipeline = ImbPipeline([
      ('tfidf', TfidfVectorizer()),
      ('smote', SMOTE(random_state=42)),  # Apply SMOTE to the training data
      ('svm', SVC(kernel='linear'))  # You can tune the kernel and other parameters
  ])

  # Train the model
  pipeline.fit(X_train, y_train)

 # Predict on the validation set
  y_pred_val = pipeline.predict(X_val)

  # Evaluate the model
  accuracy = accuracy_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred, average='macro')  # Can choose 'micro', 'macro', or 'weighted' based on what suits your needs

  print("Validation Accuracy:", accuracy)
  print("Validation F1 Score:", f1)
  print("Validation Classification Report:\n", classification_report(y_val, y_pred_val))



  #pass

  # Model is working fine, so save model
  # Note modify this with your code
  save_model(pipeline,model_dir)

  # Now Zip Model to share it
  zip_directory(model_dir, MODEL_Dis_File)

  model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

  print(model_gdrive_link)
  get_shareable_link(model_gdrive_link)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
  return model_gdrive_link

"""## Testing Method Discriminative Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

def test_Dis(test_file, MODEL_PATH,model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model

    """

    print('\n Start by downloading model')


    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link,test_model_file)
    print('Model downloaded to', test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)
    # Load the saved model
    test_df = pd.read_csv(test_file)
    print('\n Data is loaded from ', test_file)

    # Let's get the model file name & load it
    # Note you have to use same name a you used in the save

    test_model_file = os.path.join(test_model_path, 'Model_Dis', 'model.sav')

    model = load_model(test_model_file)

    # Let's do the prediction using test data
    y_pred= model.predict(test_df['comment'])

    # Now save the model output in the same test file
    # Note the name of output column, this is for the discriminative model
    test_df['out_label_model_Gen'] = y_pred

    # Now save the model output in the same output file
    test_df.to_csv(test_file, index=False)
    print('\n Output is save in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return test_file

test_Dis(test_file, MODEL_PATH,model_gdrive_link)

print(MODEL_PATH)
print(test_file)
print(model_gdrive_link)

"""## Discriminative Method  End

# Other Method/model Start
"""

import argparse

# Define argparse-like function
def parse_arguments(option):
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')
    args = parser.parse_args(args=[])
    return args

# Function to perform some action based on selected option
def perform_action(option):
    print("Performing action with option:", option)

    if option == '0':
      print('\n Okay Exiting!!! ')

    elif option == '1':
      print('\n Training Generative Model')
      model_gdrive_link = train_Gen(train_file,val_file,MODEL_Gen_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)

    elif option == '2':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Generative Model')
      model_gen_url = ''
      test_Gen(test_file, model_gen_url)

    elif option == '3':
      print('\n Training Disciminative Model')
      model_gdrive_link = train_dis(train_file,val_file,MODEL_Dis_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)
      print('\n\n Pass the URL Not Variable !!!')

    elif option == '4':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Disciminative Model')
      model_dis_url = 'https://drive.google.com/file/d/1--sJNJMLFvAz77HjiB8is8DDHOfY0FBG/view?usp=sharing'
      test_dis(test_file, MODEL_PATH, model_dis_url)

    else:
      print('Wrong Option Selected. \n\nPlease select Correct option')
      main()


def main():

    # Get option from user input
    user_option = input("0. To Exit Code\n"
                     "1. Train Model Generative\n"
                    "2. Test Model Generative\n"
                    "3. Train Model Discriminative\n"
                    "4. Test Model Discriminative\n"
                    "Enter your option: ")

    args = parse_arguments(user_option)
    option = args.option
    perform_action(option)

main()

model_dis_url = 'https://drive.google.com/file/d/1--sJNJMLFvAz77HjiB8is8DDHOfY0FBG/view?usp=sharing'
test_dis(test_file, MODEL_PATH, model_dis_url)

"""##Other Method/model End"""